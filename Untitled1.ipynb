{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv(\"imdb_master.csv\",encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>0</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>0</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>0</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>0</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review  label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...      0   \n",
       "1           1  test  This is an example of why the majority of acti...      0   \n",
       "2           2  test  First of all I hate those moronic rappers, who...      0   \n",
       "3           3  test  Not even the Beatles could write songs everyon...      0   \n",
       "4           4  test  Brass pictures (movies is not a fitting word f...      0   \n",
       "\n",
       "          file  \n",
       "0      0_2.txt  \n",
       "1  10000_4.txt  \n",
       "2  10001_1.txt  \n",
       "3  10002_3.txt  \n",
       "4  10003_3.txt  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit = 500\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_y = {e:i for i,e in enumerate(set(data['label']))}\n",
    "data['label'] = data['label'].map(dict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "punt = string.punctuation\n",
    "\n",
    "def cleaner(doc):\n",
    "    sent =  nlp(doc ,disable = ['parser', 'ner'])\n",
    "    words = [re.sub('[^0-9a-zA-Z]','',tok.lemma_.lower().strip()) for tok in sent if tok.lemma_ not in punt and tok.lemma_ != '-PRON-']\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0,limit):\n",
    "    corpus.append(cleaner(data.iloc[i]['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for sent in corpus:\n",
    "    words.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {e:i for i,e in enumerate(words)}\n",
    "itos = {i:e for i,e in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus=[]\n",
    "for sent in corpus:\n",
    "    text_corpus.append([stoi[tok] for tok in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_batches = limit // 64\n",
    "\n",
    "text_corpus = text_corpus[: no_of_batches * 64]\n",
    "len(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max count : 200\n",
      "Min Count : 200\n"
     ]
    }
   ],
   "source": [
    "minx, maxx= 10000, 0\n",
    "\n",
    "for i in range(0, len(corpus)):\n",
    "    if len(corpus[i]) < minx : minx = len(corpus[i])\n",
    "    if len(corpus[i]) > maxx : maxx = len(corpus[i])\n",
    "        \n",
    "print(\"Max count :\",maxx)\n",
    "print(\"Min Count :\",minx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(corpus, max_length):\n",
    "    \n",
    "    feature = np.zeros((len(corpus), max_length), dtype=int)\n",
    "    \n",
    "    for i, row in enumerate(corpus):\n",
    "        feature[i][-len(row):] = np.array(row)[:max_length]\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pad_sequence(text_corpus, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep LSTM-RNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_size,n_hidden,n_layers,output_size, seq_length, lstm_dropout=0.2, batch_first=True):\n",
    "        \n",
    "        super(SentRNN,self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_batch = 64\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm_net =  nn.LSTM(embed_size,n_hidden,n_layers,dropout =lstm_dropout , batch_first = batch_first)\n",
    "        self.linear = nn.Linear(seq_length * n_hidden , output_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self,X, hidden):\n",
    "        \n",
    "        embed = self.embedding(X)\n",
    "        \n",
    "        lstm_out, hidden =  self.lstm_net(embed,hidden)\n",
    "        \n",
    "        lstm_out =  lstm_out.contiguous().view(self.n_batch,-1)\n",
    "        \n",
    "        dropout = self.dropout(lstm_out)\n",
    "        \n",
    "        fc = self.linear(lstm_out)\n",
    "        \n",
    "        return fc , hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers,self.n_batch,self.n_hidden).zero_(),\n",
    "                  weight.new(self.n_layers,self.n_batch,self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.from_numpy(corpus),torch.from_numpy(np.asarray(data['label'][:len(text_corpus)])))\n",
    "train_loader = DataLoader(dataset,shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentRNN(vocab_size =  len(stoi), embed_size = 200 ,n_hidden =128 ,n_layers=3 ,output_size=3, seq_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentRNN(\n",
      "  (embedding): Embedding(9395, 200)\n",
      "  (lstm_net): LSTM(200, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (linear): Linear(in_features=25600, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "losses=[]\n",
    "for X , Y in train_loader:\n",
    "    \n",
    "    h = model.init_hidden() \n",
    "    hidden =tuple([each.data for each in h])\n",
    "    \n",
    "    #print(X.size())\n",
    "    lstm_out, hidden = model(X.long(), hidden)\n",
    "    #print(lstm_out.size())\n",
    "    cost = loss(lstm_out,  Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print(cost.item())\n",
    "    losses.append(cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 14.1139, -13.9945, -13.9592],\n",
       "        [ 24.5893, -24.4610, -24.2406],\n",
       "        [ 29.0094, -28.9005, -28.7207],\n",
       "        [ 24.7019, -24.5614, -24.3065],\n",
       "        [ 32.2401, -32.0582, -31.8641],\n",
       "        [ 13.6327, -13.5409, -13.4829],\n",
       "        [ 24.4856, -24.3404, -24.0940],\n",
       "        [ 20.3122, -20.2107, -20.0148],\n",
       "        [ 14.1162, -14.0173, -13.9585],\n",
       "        [ 24.4172, -24.2869, -24.0668],\n",
       "        [ 24.0377, -23.8727, -23.6514],\n",
       "        [ 13.8027, -13.7087, -13.6616],\n",
       "        [ 26.3495, -26.1851, -25.9515],\n",
       "        [ 14.0682, -13.9724, -13.9172],\n",
       "        [ 13.7048, -13.6167, -13.5763],\n",
       "        [ 20.0507, -19.9929, -19.7630],\n",
       "        [ 13.8485, -13.7624, -13.7157],\n",
       "        [ 16.6125, -16.4635, -16.3466],\n",
       "        [ 14.0885, -13.9997, -13.9527],\n",
       "        [ 21.5946, -21.4638, -21.2441],\n",
       "        [ 13.4843, -13.4148, -13.3463],\n",
       "        [ 22.5352, -22.3851, -22.1347],\n",
       "        [ 18.6986, -18.5995, -18.3842],\n",
       "        [ 23.4180, -23.2936, -23.0466],\n",
       "        [ 20.7612, -20.6488, -20.4520],\n",
       "        [ 19.4888, -19.3902, -19.1704],\n",
       "        [ 21.1516, -21.0672, -20.8513],\n",
       "        [ 17.7299, -17.6562, -17.4368],\n",
       "        [ 14.1553, -14.0600, -14.0161],\n",
       "        [ 22.3935, -22.2792, -22.0145],\n",
       "        [ 13.9012, -13.8082, -13.7712],\n",
       "        [ 31.6703, -31.5540, -31.3490],\n",
       "        [ 13.8814, -13.7794, -13.7490],\n",
       "        [ 13.6754, -13.6054, -13.5366],\n",
       "        [ 30.3724, -30.2978, -30.0870],\n",
       "        [ 14.1427, -14.0578, -14.0108],\n",
       "        [ 24.0734, -23.9952, -23.7383],\n",
       "        [ 14.0759, -13.9842, -13.9248],\n",
       "        [ 13.6851, -13.5697, -13.5422],\n",
       "        [ 21.0748, -20.9747, -20.8031],\n",
       "        [ 14.5083, -14.4009, -14.3720],\n",
       "        [ 13.7722, -13.6966, -13.6400],\n",
       "        [ 15.2828, -15.1702, -15.0963],\n",
       "        [ 14.4182, -14.3142, -14.2687],\n",
       "        [ 22.8663, -22.7295, -22.4760],\n",
       "        [ 13.7862, -13.6938, -13.6270],\n",
       "        [ 14.2521, -14.1688, -14.1243],\n",
       "        [ 26.2895, -26.1313, -25.9020],\n",
       "        [ 29.7770, -29.6398, -29.4868],\n",
       "        [ 13.5890, -13.5039, -13.4615],\n",
       "        [ 25.1936, -25.0611, -24.8480],\n",
       "        [ 22.8314, -22.6832, -22.4359],\n",
       "        [ 13.4643, -13.3824, -13.3374],\n",
       "        [ 13.6101, -13.5257, -13.4719],\n",
       "        [ 14.3739, -14.2778, -14.2354],\n",
       "        [ 26.9235, -26.8108, -26.6147],\n",
       "        [ 23.7328, -23.6011, -23.3735],\n",
       "        [ 13.8972, -13.8267, -13.7608],\n",
       "        [ 14.1763, -14.0887, -14.0491],\n",
       "        [ 21.4043, -21.3002, -21.0586],\n",
       "        [ 16.5926, -16.4874, -16.3132],\n",
       "        [ 13.8541, -13.7520, -13.7051],\n",
       "        [ 23.3899, -23.2198, -22.9534],\n",
       "        [ 14.0689, -13.9736, -13.9376]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
